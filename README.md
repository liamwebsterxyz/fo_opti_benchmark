# fo_opti_benchmark
In recent years, it has become increasingly important to develop efficient optimization algorithms
that can be used to solve a wide range of problems. Among these algorithms, the so-called ”first-order”
optimization algorithms are particularly popular, due to their ability to achieve good performance using
only information about the objective function and its first derivative. One of the most well-known first-
order optimization algorithms is gradient descent, which is often used as the basis for more complex
optimization methods.[1]
In this project, I will explore and implement five different first-order optimization algorithms, starting
with gradient descent and ending with the Adam method, which is one of the most widely-used optimization
algorithms in the world. I will benchmark the algorithms against challenging objective functions, and
visualize their performance to help us understand how they work and compare them against each other.
One of the key benefits of first-order optimization algorithms is their ability to handle a wide range
of different types of objective functions. For example, they can be used to optimize functions that have
many local minima, or those that are highly non-convex. Additionally, these algorithms are typically very
fast to compute, which makes them particularly useful in applications where speed is a key concern, such
as in machine learning.
Overall, the goal in this project is to gain a better understanding of first-order optimization algorithms,
and to see how they perform in practice.
